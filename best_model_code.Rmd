---
title: "Stats 101C kaggle competition classification"
author: "Maxwell Chu 405962883"
date: "2024-08-01"
output: pdf_document
---

```{r}
library(reshape)
library(tidyverse)
library(tidymodels)
library(xgboost)
library(future)
```

```{r}
plan(multisession)
```


```{r}
col_desc <- read.csv("col_descriptions.csv")
train <- read.csv("train_class.csv")
train <- train %>% select(-id, -name)
test <- read.csv("test_class.csv")

set.seed(3)
train_folds <- vfold_cv(train, v = 5, strata = winner)
```


```{r}
rf_model <- rand_forest(
  trees = tune(),
  mtry = tune(),
  min_n = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_recipe <- recipe(winner ~ ., data = train) %>%
  step_impute_median(contains("income"), contains("gdp")) %>%
  step_corr(all_numeric_predictors(), threshold = 0.95) %>%
  step_log(all_numeric_predictors(), -x0018e, offset = 1)
  
rf_wflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

rf_params <- extract_parameter_set_dials(rf_wflow) %>%
  update(trees = deg_free(c(15, 400))) %>%
  update(mtry = deg_free(c(15, 37))) %>%
  update(min_n = deg_free(c(1, 25)))
```

```{r}
# rf_fit <- rf_wflow %>% fit_resamples(resamples = train_folds)
# rf_fit %>% collect_metrics()
rf_tune_fit <- 
  rf_wflow %>%
  tune_grid(
    resamples = train_folds,
    # grid = grid_regular(rf_params, levels = 2)
    grid = grid_random(rf_params, size = 100)
  )
```

```{r}
rf_res <- rf_tune_fit %>% show_best(n = 1000, metric = "accuracy")
ggplot(data = rf_res) + geom_point(aes(x = min_n, y = mean))
ggplot(data = rf_res) + geom_point(aes(x = mtry, y = mean))
ggplot(data = rf_res) + geom_point(aes(x = trees, y = mean))
rf_res
```


```{r}
# ggplot(data = train) + geom_histogram(aes(x = income_per_cap_2016), bins = 50)
# ggplot(data = train) + geom_histogram(aes(x = income_per_cap_2017))
# ggplot(data = train) + geom_histogram(aes(x = income_per_cap_2018))
# ggplot(data = train) + geom_histogram(aes(x = income_per_cap_2019))
# ggplot(data = train) + geom_histogram(aes(x = income_per_cap_2020))

train_gdp <- train %>% select(x0086e)
summary(train_gdp)
train_gdp <- melt(train_gdp)
ggplot(aes(x = log(value), fill = variable), data = train_gdp) + geom_density(alpha = 0.25) #+ scale_x_continuous(limits = c(NA, 3000000))
# mtry 14 min_n 1 trees 700
```

```{r}
rf_best_model <- rf_tune_fit %>% select_best(metric = "accuracy")
rf_final_wflow <- rf_wflow %>% finalize_workflow(rf_best_model)
rf_final_fit <- rf_final_wflow %>% fit(data = train)
rf_final_pred <- rf_final_fit %>% predict(new_data = test)
rf_final_res <- bind_cols(id = test$id, winner = rf_final_pred$.pred_class)
```

```{r}
rf_final_res
write_csv(rf_final_res, "submission.csv")
```

```{r}
bt_model <- boost_tree(
  trees = tune(),
  mtry = tune(),
  min_n = 2,
  learn_rate = tune(),
  tree_depth = tune(),
  sample_size = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

bt_recipe <- recipe(winner ~ ., data = train) %>%
  step_impute_median(contains("income"), contains("gdp")) %>%
  step_log(all_numeric_predictors(), -x0018e, offset = 1)
  
bt_wflow <- workflow() %>%
  add_model(bt_model) %>%
  add_recipe(bt_recipe)

bt_params <- extract_parameter_set_dials(bt_wflow) %>%
  update(mtry = finalize(mtry(), x = train %>% select(-winner))) %>%
  update(trees = deg_free(c(5, 300))) %>%
  # update(min_n = deg_free(c(2, 15))) %>%
  update(learn_rate = learn_rate(range = c(-1.30103, -0.60206))) %>%
  update(tree_depth = deg_free(c(7, 15)))
```


```{r}
# bt_fit <- bt_wflow %>% fit_resamples(resamples = train_folds)
# bt_fit %>% collect_metrics()

bt_tune_fit <-
  bt_wflow %>%
  tune_grid(
    resamples = train_folds,
    grid = grid_random(bt_params, size = 50)
    # grid = grid_regular(bt_params, levels = 4)
  )

bt_tune_bayes_fit <- 
  bt_wflow %>%
  tune_bayes(
    resamples = train_folds,
    param_info = bt_params,
    iter = 50,
    initial = bt_tune_fit,
    metrics = metric_set(roc_auc, accuracy),
    objective = exp_improve(trade_off = 0.03),
    control = control_bayes(uncertain = 5)

  )
```

```{r}
bt_res <- bt_tune_bayes_fit %>% show_best(n = 1000, metric = "accuracy") #%>% filter(sample_size < 0.7)
ggplot(data = bt_res) + geom_point(aes(x = mtry, y = mean))
ggplot(data = bt_res) + geom_point(aes(x = trees, y = mean))
ggplot(data = bt_res) + geom_point(aes(x = learn_rate, y = mean))
ggplot(data = bt_res) + geom_point(aes(x = tree_depth, y = mean))
ggplot(data = bt_res) + geom_point(aes(x = sample_size, y = mean))
bt_res

```

```{r}
bt_best_model <- bt_tune_fit %>% select_best(metric = "roc_auc")
bt_final_wflow <- bt_wflow %>% finalize_workflow(bt_best_model)
bt_final_fit <- bt_final_wflow %>% fit(data = train)
bt_final_pred <- bt_final_fit %>% predict(new_data = test)
bt_final_res <- bind_cols(id = test$id, winner = bt_final_pred$.pred_class)
```

```{r}
bt_final_res
write_csv(bt_final_res, "submission.csv")
```